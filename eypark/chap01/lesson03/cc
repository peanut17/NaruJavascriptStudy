Spring Boot를 사용하여 Apache Kafka의 데이터를 HDFS에 저장하는 애플리케이션을 구현할 수 있습니다. 이를 위해 다음과 같은 단계를 따릅니다.

1. **Spring Boot 프로젝트 생성**
   Spring Initializr를 사용하여 새로운 Spring Boot 프로젝트를 생성합니다. 필요한 의존성으로는 Kafka와 HDFS 클라이언트 라이브러리가 포함되어야 합니다.

2. **필요한 의존성 추가**
   `pom.xml` 파일에 Kafka와 HDFS 클라이언트 관련 의존성을 추가합니다.

   ```xml
   <dependencies>
       <!-- Spring Boot Kafka -->
       <dependency>
           <groupId>org.springframework.boot</groupId>
           <artifactId>spring-boot-starter-kafka</artifactId>
       </dependency>

       <!-- Hadoop HDFS -->
       <dependency>
           <groupId>org.apache.hadoop</groupId>
           <artifactId>hadoop-hdfs</artifactId>
           <version>3.3.1</version>
       </dependency>

       <!-- Spring Boot Starter Web -->
       <dependency>
           <groupId>org.springframework.boot</groupId>
           <artifactId>spring-boot-starter-web</artifactId>
       </dependency>
   </dependencies>
   ```

3. **Kafka Consumer 설정**
   `application.yml` 파일에 Kafka 설정을 추가합니다.

   ```yaml
   spring:
     kafka:
       bootstrap-servers: localhost:9092
       consumer:
         group-id: your-consumer-group
         auto-offset-reset: earliest
         key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
         value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
   ```

4. **Kafka Consumer 구현**
   Kafka 메시지를 소비하고 HDFS에 저장하는 로직을 구현합니다.

   ```java
   import org.apache.hadoop.conf.Configuration;
   import org.apache.hadoop.fs.FileSystem;
   import org.apache.hadoop.fs.Path;
   import org.springframework.kafka.annotation.KafkaListener;
   import org.springframework.stereotype.Service;

   import java.io.BufferedWriter;
   import java.io.OutputStreamWriter;
   import java.net.URI;
   import java.nio.charset.StandardCharsets;
   import java.util.List;

   @Service
   public class KafkaConsumerService {

       private final FileSystem hdfs;

       public KafkaConsumerService() throws Exception {
           Configuration configuration = new Configuration();
           configuration.set("fs.defaultFS", "hdfs://your_hadoop_cluster");
           this.hdfs = FileSystem.get(new URI("hdfs://your_hadoop_cluster"), configuration);
       }

       @KafkaListener(topics = "#{'${spring.kafka.consumer.topics}'.split(',')}", groupId = "${spring.kafka.consumer.group-id}")
       public void listen(List<String> messages, String topic) {
           messages.forEach(message -> {
               try {
                   Path path = new Path("/user/your_user/" + topic + "/" + System.currentTimeMillis() + ".txt");
                   try (BufferedWriter br = new BufferedWriter(new OutputStreamWriter(hdfs.create(path), StandardCharsets.UTF_8))) {
                       br.write(message);
                   }
               } catch (Exception e) {
                   e.printStackTrace();
               }
           });
       }
   }
   ```

5. **Spring Boot Application 클래스**
   Spring Boot 애플리케이션 클래스를 작성하여 애플리케이션을 실행합니다.

   ```java
   import org.springframework.boot.SpringApplication;
   import org.springframework.boot.autoconfigure.SpringBootApplication;

   @SpringBootApplication
   public class KafkaToHdfsApplication {

       public static void main(String[] args) {
           SpringApplication.run(KafkaToHdfsApplication.class, args);
       }
   }
   ```

6. **실행**
   위의 코드가 완료되면 Spring Boot 애플리케이션을 실행하여 Kafka의 60개 토픽 데이터를 실시간으로 HDFS에 저장할 수 있습니다.

이 방법을 사용하면 Spring Boot 애플리케이션에서 Kafka의 데이터를 지속적으로 소비하고, 해당 데이터를 HDFS에 저장하는 시스템을 구축할 수 있습니다. 설정 파일과 코드의 URI, 경로, 및 설정 값은 실제 환경에 맞게 조정해야 합니다.Spring Boot를 사용하여 Apache Kafka의 데이터를 HDFS에 저장하는 애플리케이션을 구현할 수 있습니다. 이를 위해 다음과 같은 단계를 따릅니다.

1. **Spring Boot 프로젝트 생성**
   Spring Initializr를 사용하여 새로운 Spring Boot 프로젝트를 생성합니다. 필요한 의존성으로는 Kafka와 HDFS 클라이언트 라이브러리가 포함되어야 합니다.

2. **필요한 의존성 추가**
   `pom.xml` 파일에 Kafka와 HDFS 클라이언트 관련 의존성을 추가합니다.

   ```xml
   <dependencies>
       <!-- Spring Boot Kafka -->
       <dependency>
           <groupId>org.springframework.boot</groupId>
           <artifactId>spring-boot-starter-kafka</artifactId>
       </dependency>

       <!-- Hadoop HDFS -->
       <dependency>
           <groupId>org.apache.hadoop</groupId>
           <artifactId>hadoop-hdfs</artifactId>
           <version>3.3.1</version>
       </dependency>

       <!-- Spring Boot Starter Web -->
       <dependency>
           <groupId>org.springframework.boot</groupId>
           <artifactId>spring-boot-starter-web</artifactId>
       </dependency>
   </dependencies>
   ```

3. **Kafka Consumer 설정**
   `application.yml` 파일에 Kafka 설정을 추가합니다.

   ```yaml
   spring:
     kafka:
       bootstrap-servers: localhost:9092
       consumer:
         group-id: your-consumer-group
         auto-offset-reset: earliest
         key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
         value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
   ```

4. **Kafka Consumer 구현**
   Kafka 메시지를 소비하고 HDFS에 저장하는 로직을 구현합니다.

   ```java
   import org.apache.hadoop.conf.Configuration;
   import org.apache.hadoop.fs.FileSystem;
   import org.apache.hadoop.fs.Path;
   import org.springframework.kafka.annotation.KafkaListener;
   import org.springframework.stereotype.Service;

   import java.io.BufferedWriter;
   import java.io.OutputStreamWriter;
   import java.net.URI;
   import java.nio.charset.StandardCharsets;
   import java.util.List;

   @Service
   public class KafkaConsumerService {

       private final FileSystem hdfs;

       public KafkaConsumerService() throws Exception {
           Configuration configuration = new Configuration();
           configuration.set("fs.defaultFS", "hdfs://your_hadoop_cluster");
           this.hdfs = FileSystem.get(new URI("hdfs://your_hadoop_cluster"), configuration);
       }

       @KafkaListener(topics = "#{'${spring.kafka.consumer.topics}'.split(',')}", groupId = "${spring.kafka.consumer.group-id}")
       public void listen(List<String> messages, String topic) {
           messages.forEach(message -> {
               try {
                   Path path = new Path("/user/your_user/" + topic + "/" + System.currentTimeMillis() + ".txt");
                   try (BufferedWriter br = new BufferedWriter(new OutputStreamWriter(hdfs.create(path), StandardCharsets.UTF_8))) {
                       br.write(message);
                   }
               } catch (Exception e) {
                   e.printStackTrace();
               }
           });
       }
   }
   ```

5. **Spring Boot Application 클래스**
   Spring Boot 애플리케이션 클래스를 작성하여 애플리케이션을 실행합니다.

   ```java
   import org.springframework.boot.SpringApplication;
   import org.springframework.boot.autoconfigure.SpringBootApplication;

   @SpringBootApplication
   public class KafkaToHdfsApplication {

       public static void main(String[] args) {
           SpringApplication.run(KafkaToHdfsApplication.class, args);
       }
   }
   ```

6. **실행**
   위의 코드가 완료되면 Spring Boot 애플리케이션을 실행하여 Kafka의 60개 토픽 데이터를 실시간으로 HDFS에 저장할 수 있습니다.

이 방법을 사용하면 Spring Boot 애플리케이션에서 Kafka의 데이터를 지속적으로 소비하고, 해당 데이터를 HDFS에 저장하는 시스템을 구축할 수 있습니다. 설정 파일과 코드의 URI, 경로, 및 설정 값은 실제 환경에 맞게 조정해야 합니다.
