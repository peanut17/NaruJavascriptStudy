Spring Boot를 사용하여 Apache Kafka의 60개 토픽 데이터를 HDFS에 저장하는 애플리케이션을 자세히 구현하는 방법을 제공하겠습니다.

### 1. Spring Boot 프로젝트 생성
Spring Initializr를 사용하여 새로운 Spring Boot 프로젝트를 생성합니다. 필요한 의존성으로는 Kafka와 HDFS 클라이언트 라이브러리가 포함되어야 합니다.

### 2. `pom.xml` 의존성 추가
`pom.xml` 파일에 Kafka와 HDFS 클라이언트 관련 의존성을 추가합니다.

```xml
<dependencies>
    <!-- Spring Boot Kafka -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-kafka</artifactId>
    </dependency>

    <!-- Hadoop HDFS -->
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-hdfs</artifactId>
        <version>3.3.1</version>
    </dependency>

    <!-- Spring Boot Starter Web -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
</dependencies>
```

### 3. `application.yml` 설정 추가
`src/main/resources/application.yml` 파일에 Kafka 설정을 추가합니다.

```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: your-consumer-group
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      topics: topic1,topic2,topic3,topic4,topic5,topic6,topic7,topic8,topic9,topic10,topic11,topic12,topic13,topic14,topic15,topic16,topic17,topic18,topic19,topic20,topic21,topic22,topic23,topic24,topic25,topic26,topic27,topic28,topic29,topic30,topic31,topic32,topic33,topic34,topic35,topic36,topic37,topic38,topic39,topic40,topic41,topic42,topic43,topic44,topic45,topic46,topic47,topic48,topic49,topic50,topic51,topic52,topic53,topic54,topic55,topic56,topic57,topic58,topic59,topic60

hadoop:
  fs-uri: hdfs://your_hadoop_cluster
  user: your_user
```

### 4. Kafka Consumer 구현
Kafka 메시지를 소비하고 HDFS에 저장하는 서비스를 구현합니다.

```java
package com.example.kafkatohdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

import javax.annotation.PostConstruct;
import java.io.BufferedWriter;
import java.io.OutputStreamWriter;
import java.net.URI;
import java.nio.charset.StandardCharsets;
import java.util.List;

@Service
public class KafkaConsumerService {

    @Value("${hadoop.fs-uri}")
    private String hdfsUri;

    @Value("${hadoop.user}")
    private String hdfsUser;

    private FileSystem hdfs;

    @PostConstruct
    public void init() throws Exception {
        Configuration configuration = new Configuration();
        configuration.set("fs.defaultFS", hdfsUri);
        this.hdfs = FileSystem.get(new URI(hdfsUri), configuration, hdfsUser);
    }

    @KafkaListener(topics = "#{'${spring.kafka.consumer.topics}'.split(',')}", groupId = "${spring.kafka.consumer.group-id}")
    public void listen(List<String> messages, String topic) {
        messages.forEach(message -> {
            try {
                Path path = new Path("/user/" + hdfsUser + "/" + topic + "/" + System.currentTimeMillis() + ".txt");
                try (BufferedWriter br = new BufferedWriter(new OutputStreamWriter(hdfs.create(path), StandardCharsets.UTF_8))) {
                    br.write(message);
                }
            } catch (Exception e) {
                e.printStackTrace();
            }
        });
    }
}
```

### 5. Spring Boot Application 클래스
Spring Boot 애플리케이션 클래스를 작성하여 애플리케이션을 실행합니다.

```java
package com.example.kafkatohdfs;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class KafkaToHdfsApplication {

    public static void main(String[] args) {
        SpringApplication.run(KafkaToHdfsApplication.class, args);
    }
}
```

### 6. 실행
위의 코드가 완료되면 Spring Boot 애플리케이션을 실행하여 Kafka의 60개 토픽 데이터를 실시간으로 HDFS에 저장할 수 있습니다.

다음 명령어를 사용하여 Spring Boot 애플리케이션을 실행합니다.

```bash
mvn spring-boot:run
```

이렇게 하면 Spring Boot 애플리케이션이 실행되며, Kafka의 데이터를 지속적으로 소비하여 HDFS에 저장합니다. 각 토픽의 데이터를 별도로 저장하도록 설정하였기 때문에 관리가 용이합니다.
