Spring Boot를 사용하여 Kafka 데이터를 HDFS에 저장하고, 저장된 데이터를 Hive 테이블에 삽입하는 애플리케이션을 자세히 구현하겠습니다.

### 1. Spring Boot 프로젝트 생성

Spring Initializr를 사용하여 새로운 Spring Boot 프로젝트를 생성합니다. 필요한 의존성으로는 Kafka, Hadoop HDFS, 그리고 Hive JDBC가 포함되어야 합니다.

### 2. `pom.xml` 의존성 추가

`pom.xml` 파일에 필요한 의존성을 추가합니다.

```xml
<dependencies>
    <!-- Spring Boot Kafka -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-kafka</artifactId>
    </dependency>

    <!-- Hadoop HDFS -->
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-hdfs</artifactId>
        <version>3.3.1</version>
    </dependency>

    <!-- Hive JDBC -->
    <dependency>
        <groupId>org.apache.hive</groupId>
        <artifactId>hive-jdbc</artifactId>
        <version>3.1.2</version>
    </dependency>

    <!-- Spring Boot Starter Web -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
</dependencies>
```

### 3. `application.yml` 설정 추가

`src/main/resources/application.yml` 파일에 Kafka, Hadoop, 및 Hive 설정을 추가합니다.

```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: your-consumer-group
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      topics: topic1,topic2,topic3,topic4,topic5,topic6,topic7,topic8,topic9,topic10,topic11,topic12,topic13,topic14,topic15,topic16,topic17,topic18,topic19,topic20,topic21,topic22,topic23,topic24,topic25,topic26,topic27,topic28,topic29,topic30,topic31,topic32,topic33,topic34,topic35,topic36,topic37,topic38,topic39,topic40,topic41,topic42,topic43,topic44,topic45,topic46,topic47,topic48,topic49,topic50,topic51,topic52,topic53,topic54,topic55,topic56,topic57,topic58,topic59,topic60

hadoop:
  fs-uri: hdfs://your_hadoop_cluster
  user: your_user

hive:
  jdbc-url: jdbc:hive2://your_hive_server:10000/default
  username: your_hive_user
  password: your_hive_password
```

### 4. Kafka Consumer 구현

Kafka 메시지를 소비하고 HDFS에 저장한 후, Hive 테이블에 삽입하는 서비스를 구현합니다.

```java
package com.example.kafkatohdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

import javax.annotation.PostConstruct;
import java.io.BufferedWriter;
import java.io.OutputStreamWriter;
import java.net.URI;
import java.nio.charset.StandardCharsets;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.Statement;
import java.util.List;

@Service
public class KafkaConsumerService {

    @Value("${hadoop.fs-uri}")
    private String hdfsUri;

    @Value("${hadoop.user}")
    private String hdfsUser;

    @Value("${hive.jdbc-url}")
    private String hiveJdbcUrl;

    @Value("${hive.username}")
    private String hiveUsername;

    @Value("${hive.password}")
    private String hivePassword;

    private FileSystem hdfs;

    @PostConstruct
    public void init() throws Exception {
        Configuration configuration = new Configuration();
        configuration.set("fs.defaultFS", hdfsUri);
        this.hdfs = FileSystem.get(new URI(hdfsUri), configuration, hdfsUser);
    }

    @KafkaListener(topics = "#{'${spring.kafka.consumer.topics}'.split(',')}", groupId = "${spring.kafka.consumer.group-id}")
    public void listen(List<String> messages, String topic) {
        messages.forEach(message -> {
            try {
                String hdfsPath = "/user/" + hdfsUser + "/" + topic + "/" + System.currentTimeMillis() + ".txt";
                Path path = new Path(hdfsPath);
                try (BufferedWriter br = new BufferedWriter(new OutputStreamWriter(hdfs.create(path), StandardCharsets.UTF_8))) {
                    br.write(message);
                }
                insertIntoHive(topic, hdfsPath);
            } catch (Exception e) {
                e.printStackTrace();
            }
        });
    }

    private void insertIntoHive(String topic, String hdfsPath) throws Exception {
        Class.forName("org.apache.hive.jdbc.HiveDriver");
        try (Connection conn = DriverManager.getConnection(hiveJdbcUrl, hiveUsername, hivePassword);
             Statement stmt = conn.createStatement()) {

            String sql = String.format("LOAD DATA INPATH '%s' INTO TABLE %s", hdfsPath, topic);
            stmt.execute(sql);
        }
    }
}
```

### 5. Spring Boot Application 클래스

Spring Boot 애플리케이션 클래스를 작성하여 애플리케이션을 실행합니다.

```java
package com.example.kafkatohdfs;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class KafkaToHdfsApplication {

    public static void main(String[] args) {
        SpringApplication.run(KafkaToHdfsApplication.class, args);
    }
}
```

### 6. Hive 테이블 생성

각 Kafka 토픽에 대응하는 Hive 테이블을 생성합니다. 예를 들어, `topic1`에 대한 테이블을 생성하려면 다음과 같은 HiveQL을 사용합니다.

```sql
CREATE TABLE topic1 (
    message STRING
)
STORED AS TEXTFILE;
```

### 7. 실행

위의 코드가 완료되면 Spring Boot 애플리케이션을 실행하여 Kafka의 60개 토픽 데이터를 실시간으로 HDFS에 저장하고, 저장된 데이터를 Hive 테이블에 삽입할 수 있습니다.

다음 명령어를 사용하여 Spring Boot 애플리케이션을 실행합니다.

```bash
mvn spring-boot:run
```

이렇게 하면 Spring Boot 애플리케이션이 실행되며, Kafka의 데이터를 지속적으로 소비하여 HDFS에 저장하고, Hive 테이블에 삽입합니다. 각 토픽의 데이터를 별도로 저장하고 삽입하도록 설정하였기 때문에 관리가 용이합니다.
