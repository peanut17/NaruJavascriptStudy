Apache Kafka의 60개 토픽 데이터를 Apache Spark를 이용해 HDFS에 저장하는 방법은 다음과 같습니다. 각 토픽 데이터를 별도로 읽어와 HDFS에 저장할 수 있도록 합니다. Spark Structured Streaming을 이용하면 쉽게 구현할 수 있습니다.

1. **필요한 라이브러리 설치**
   먼저 필요한 라이브러리를 설치합니다. `pyspark`, `kafka-python`, `hadoop-client` 등의 라이브러리를 설치합니다.

   ```bash
   pip install pyspark kafka-python hadoop-client
   ```

2. **Spark Session 생성**
   PySpark 프로그램에서 Spark Session을 생성합니다.

   ```python
   from pyspark.sql import SparkSession

   spark = SparkSession.builder \
       .appName("KafkaToHDFS") \
       .getOrCreate()
   ```

3. **Kafka 데이터 스트림 읽기**
   60개 토픽의 데이터를 읽기 위해 Spark Structured Streaming을 사용합니다.

   ```python
   kafka_bootstrap_servers = "localhost:9092"
   kafka_topics = [f"topic{i}" for i in range(1, 61)]  # 60개의 토픽 이름 생성
   kafka_topics_str = ",".join(kafka_topics)

   df = spark \
       .readStream \
       .format("kafka") \
       .option("kafka.bootstrap.servers", kafka_bootstrap_servers) \
       .option("subscribe", kafka_topics_str) \
       .load()
   ```

   Kafka 데이터는 기본적으로 바이너리 형식이므로 이를 적절한 문자열 형식으로 변환해야 합니다.

   ```python
   from pyspark.sql.functions import col, from_json
   from pyspark.sql.types import StringType

   df = df.selectExpr("CAST(topic AS STRING)", "CAST(key AS STRING)", "CAST(value AS STRING)")
   ```

4. **HDFS에 데이터 저장**
   이제 Kafka에서 읽은 데이터를 HDFS에 저장하는 코드를 작성합니다. 토픽별로 데이터를 분리해서 저장할 수도 있습니다.

   ```python
   def write_to_hdfs(topic):
       topic_df = df.filter(col("topic") == topic)
       query = topic_df.writeStream \
           .outputMode("append") \
           .format("parquet") \
           .option("path", f"hdfs://your_hadoop_cluster/user/your_user/{topic}") \
           .option("checkpointLocation", f"hdfs://your_hadoop_cluster/user/your_user/checkpoints/{topic}") \
           .start()
       return query

   queries = [write_to_hdfs(topic) for topic in kafka_topics]
   
   for query in queries:
       query.awaitTermination()
   ```

   여기서 `outputMode`는 스트리밍 데이터의 출력 모드를 지정하며, `append` 모드는 새로운 데이터가 있을 때마다 추가하는 방식입니다. `format`은 데이터를 저장할 파일 형식을 지정하며, `parquet` 형식을 사용합니다. `path`와 `checkpointLocation`은 각각 데이터를 저장할 경로와 체크포인트를 저장할 경로를 지정합니다.

이제 이 코드를 실행하면 60개 토픽의 Kafka 데이터가 실시간으로 HDFS에 저장됩니다. HDFS 경로와 Kafka 설정은 실제 사용 환경에 맞게 조정해야 합니다. 각 토픽의 데이터를 별도로 저장하도록 설정하였기 때문에 관리가 용이합니다.
