Spring WebFlux를 사용하여 Kafka에서 데이터를 비동기적으로 소비하고, HDFS에 데이터를 저장하는 코드를 작성할 수 있습니다. WebFlux는 비동기 및 논블로킹 방식으로 작동하므로, Kafka와 HDFS 클라이언트도 비동기식으로 구성해야 합니다.

다음은 Spring WebFlux와 함께 Kafka 데이터를 비동기적으로 소비하고, HDFS에 데이터를 저장하는 예제입니다.

### 1. 프로젝트 설정

**1.1. 의존성 추가**
Maven을 사용하여 프로젝트의 `pom.xml`에 필요한 의존성을 추가합니다.
```xml
<dependencies>
    <!-- Spring Boot WebFlux -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-webflux</artifactId>
    </dependency>
    <!-- Spring Boot Kafka -->
    <dependency>
        <groupId>org.springframework.kafka</groupId>
        <artifactId>spring-kafka</artifactId>
    </dependency>
    <!-- Hadoop HDFS -->
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>3.3.1</version>
    </dependency>
    <!-- Jackson for JSON processing -->
    <dependency>
        <groupId>com.fasterxml.jackson.core</groupId>
        <artifactId>jackson-databind</artifactId>
    </dependency>
</dependencies>
```

### 2. Kafka 설정

**2.1. Kafka Consumer 설정**
```java
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.annotation.EnableKafka;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;

import java.util.HashMap;
import java.util.Map;

@Configuration
@EnableKafka
public class KafkaConsumerConfig {

    @Bean
    public ConsumerFactory<String, String> consumerFactory() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "group_id");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        return new DefaultKafkaConsumerFactory<>(props);
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());
        return factory;
    }
}
```

### 3. HDFS 설정

**3.1. HDFS 클라이언트 설정**
```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.Path;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Mono;

import java.io.IOException;
import java.net.URI;

@Service
public class HDFSService {

    private FileSystem fileSystem;

    public HDFSService() throws IOException {
        Configuration configuration = new Configuration();
        configuration.set("fs.defaultFS", "hdfs://namenode:9000");
        fileSystem = FileSystem.get(URI.create("hdfs://namenode:9000"), configuration);
    }

    public Mono<Void> writeData(String filePath, String data) {
        return Mono.fromRunnable(() -> {
            Path path = new Path(filePath);
            try (FSDataOutputStream outputStream = fileSystem.create(path, true)) {
                outputStream.writeUTF(data);
            } catch (IOException e) {
                e.printStackTrace();
            }
        });
    }

    public void close() throws IOException {
        fileSystem.close();
    }
}
```

### 4. Kafka Listener 및 데이터 처리

**4.1. Kafka Listener 작성**
```java
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Mono;

import java.io.IOException;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

@Service
public class KafkaConsumerService {

    private final HDFSService hdfsService;
    private final ObjectMapper objectMapper;
    private final Map<String, JsonNode> lastStateMap;

    public KafkaConsumerService(HDFSService hdfsService) {
        this.hdfsService = hdfsService;
        this.objectMapper = new ObjectMapper();
        this.lastStateMap = new ConcurrentHashMap<>();
    }

    @KafkaListener(topics = {"topic1", "topic2", "topic3", "topic4", "topic5",
                             "topic6", "topic7", "topic8", "topic9", "topic10",
                             "topic11", "topic12", "topic13", "topic14", "topic15",
                             "topic16", "topic17", "topic18", "topic19", "topic20"}, 
                             groupId = "group_id")
    public void listen(String message) {
        Mono.fromRunnable(() -> {
            try {
                JsonNode jsonNode = objectMapper.readTree(message);
                String key = jsonNode.get("key").asText();
                JsonNode after = jsonNode.get("after");

                JsonNode before = lastStateMap.get(key);
                lastStateMap.put(key, after);

                if (before != null && !before.equals(after)) {
                    String diff = createDiff(before, after);
                    hdfsService.writeData("/path/to/hdfs/" + key + ".txt", diff).subscribe();
                } else if (before == null) {
                    hdfsService.writeData("/path/to/hdfs/" + key + ".txt", after.toString()).subscribe();
                }

            } catch (Exception e) {
                e.printStackTrace();
            }
        }).subscribe();
    }

    private String createDiff(JsonNode before, JsonNode after) {
        // Implement your logic to create a diff between before and after JSON objects
        return "Diff: " + before.toString() + " -> " + after.toString();
    }
}
```

### 요약

1. `pom.xml` 파일에 필요한 의존성을 추가합니다.
2. Kafka Consumer 설정을 구성합니다.
3. HDFS 클라이언트를 설정합니다.
4. Kafka Listener를 작성하여 비동기적으로 데이터를 소비하고, 변경된 데이터를 HDFS에 저장합니다.

이러한 단계를 따르면 Spring WebFlux를 사용하여 Kafka 토픽에서 데이터를 비동기적으로 소비하고, HDFS에 데이터를 저장할 수 있습니다. `createDiff` 메서드는 `before`와 `after` JSON 객체 간의 차이점을 계산하여 diff 문자열을 생성하는 부분으로, 필요에 맞게 구현해야 합니다.
