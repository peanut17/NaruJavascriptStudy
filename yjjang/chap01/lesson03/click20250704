
ì¢‹ìŠµë‹ˆë‹¤!
ClickHouseì— ì‹¤ì‹œê°„ UPDATEë¥¼ ì ìš©í•˜ë ¤ëŠ” êµ¬ì¡°ì—ì„œ,
**PySparkì˜ foreachPartition()**ì„ ì‚¬ìš©í•´ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ëŠ” ê±´ ì•„ì£¼ ì‹¤ìš©ì ì¸ ë°©ì‹ì…ë‹ˆë‹¤.


---

âœ… ëª©í‘œ êµ¬ì¡° ìš”ì•½

> Kafka â†’ PySpark â†’ foreachPartition() â†’ ClickHouseì— ALTER TABLE ... UPDATE
(ë‹¨, mutation ê³¼ë¶€í•˜ë¥¼ ë°©ì§€í•˜ë©° ë³‘ë ¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)




---

âœ… ìµœì  êµ¬ì¡° ì˜ˆì‹œ: foreachPartition() ì•ˆì—ì„œ UPDATE

def update_partition(rows):
    from clickhouse_driver import Client
    client = Client(host="your-clickhouse-host", database="your_db")

    for row in rows:
        table = row['table']  # í…Œì´ë¸”ëª…
        before = row['before']  # í‚¤ ê°’
        after = row['after']  # ì—…ë°ì´íŠ¸í•  ë°ì´í„°

        if not before or not after:
            continue

        where_clause = " AND ".join(f"{k} = {format_val(before[k])}" for k in before)
        set_clause = ", ".join(f"{k} = {format_val(v)}" for k, v in after.items())

        sql = f"ALTER TABLE {table} UPDATE {set_clause} WHERE {where_clause}"

        try:
            client.execute(sql)
        except Exception as e:
            print(f"[ERROR] UPDATE failed: {e} \nSQL: {sql}")


---

ğŸ”§ ë³´ì¡° í•¨ìˆ˜: ê°’ escape

def format_val(v):
    if v is None:
        return 'NULL'
    if isinstance(v, str):
        return f"'{v.replace(\"'\", \"''\")}'"
    return str(v)


---

ğŸ” ì—°ê²°: PySpark Streaming

json_df.writeStream \
    .foreachBatch(lambda df, _: df.foreachPartition(update_partition)) \
    .outputMode("update") \
    .trigger(processingTime="5 seconds") \
    .start()


---

âœ… ë³‘ë ¬ ìµœì í™” íŒ

í•­ëª©	ì¶”ì²œ

.repartition(10~50)	mutation ê³¼ë¶€í•˜ ë°©ì§€ìš© ë³‘ë ¬ë„ ì œí•œ
trigger("5 seconds")	batch ì£¼ê¸° ì œí•œ
OPTIMIZE TABLE ... FINAL	mutation ì¶•ì  ë°©ì§€
mutation queue ê°ì‹œ	system.mutations í…Œì´ë¸”ë¡œ ëª¨ë‹ˆí„°ë§



---

âœ… ë³‘ë ¬ ì•ˆì „ì„± ì›ì¹™

Client()ëŠ” ë°˜ë“œì‹œ foreachPartition() ë‚´ë¶€ì—ì„œ ìƒì„±

ì™¸ë¶€ ì •ì˜ ì‹œ: âŒ cannot pickle '_thread.lock' ì˜¤ë¥˜ ë°œìƒ

try/exceptë¡œ mutation ì‹¤íŒ¨ ë¡œê¹…



---

ğŸ”¥ ë³´ë„ˆìŠ¤: mutation ì¿¼ë¦¬ ìˆ˜ ì¤„ì´ê¸° (ì˜µì…˜)

id IN (...) êµ¬ì¡°ë¡œ UPDATE ë¬¶ëŠ” ë¡œì§ë„ ê°€ëŠ¥ â†’ mutation ê°œìˆ˜ ì¤„ì´ê¸°
(ì›í•˜ì‹œë©´ ì´ êµ¬ì¡°ë„ ì œê³µí•´ë“œë¦´ê²Œìš”)


---

í•„ìš”í•˜ì‹œë©´:

DELETE + INSERT êµ¬ì¡°ì™€ í˜¼í•© ì²˜ë¦¬

mutation queue ëª¨ë‹ˆí„°ë§ SQL

mutation ìë™ ë³‘í•© ìŠ¤ì¼€ì¤„ë§


ë„ í•¨ê»˜ êµ¬ì„±í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì§€ê¸ˆ foreachPartition + Client + mutation ê´€ë¦¬ ì¡°í•©ì€ ClickHouse ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ì— ê°€ì¥ ê°•ë ¥í•œ ë°©ì‹ì…ë‹ˆë‹¤ ğŸ’ª







ì¢‹ìŠµë‹ˆë‹¤!
.repartition()ì€ Sparkì˜ DataFrame ì—°ì‚°ì´ê³ ,
foreachPartition()ì€ writeStream ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ íŒŒí‹°ì…˜ë³„ë¡œ ë™ì‘í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
ì¦‰, foreachPartition() ì „ì— .repartition(n)ì„ DataFrameì— ì ìš©í•˜ë©´ ë©ë‹ˆë‹¤.


---

âœ… ì ìš© ìœ„ì¹˜ ì˜ˆì‹œ

# 1. Kafka â†’ JSON ì¶”ì¶œ
parsed_df = kafka_df.selectExpr("CAST(value AS STRING) as json_str") \
    .select(from_json(col("json_str"), schema).alias("data")) \
    .select("data.*") \
    .filter(col("op_code") == "U")

# 2. ğŸ’¡ ë³‘ë ¬ íŒŒí‹°ì…˜ ìˆ˜ ëª…ì‹œ (e.g. 20ê°œë¡œ)
repartitioned_df = parsed_df.repartition(20)

# 3. ì‹¤ì‹œê°„ writeStream + foreachPartition
repartitioned_df.writeStream \
    .foreachBatch(lambda df, _: df.foreachPartition(update_partition)) \
    .outputMode("update") \
    .trigger(processingTime="5 seconds") \
    .start()


---

âœ… í•µì‹¬ ìš”ì•½

í•­ëª©	ì„¤ëª…

.repartition(n)	DataFrameì— ì ìš© (partition ìˆ˜ë¥¼ nê°œë¡œ ì¬ì„¤ì •)
.foreachPartition(...)	ê° partitionì— ëŒ€í•´ ë³‘ë ¬ ì‹¤í–‰
ë³‘ë ¬ íš¨ê³¼	nê°œì˜ Spark taskê°€ ê°ê° ë³‘ë ¬ë¡œ update_partition() ìˆ˜í–‰



---

ğŸ¯ ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼ í™•ì¸ íŒ

update_partition() ì•ˆì— ë‹¤ìŒ ë¡œê·¸ ì¶”ê°€:

import socket
print(f"[{socket.gethostname()}] Partition ì‹¤í–‰ë¨")

ë˜ëŠ” Spark UI (localhost:4040) â†’ Stagesì—ì„œ ì‹¤ì œ task ê°œìˆ˜ í™•ì¸


---

âš ï¸ ì£¼ì˜ì‚¬í•­

ë¬¸ì œ	í•´ê²° ë°©ë²•

mutation ë„ˆë¬´ ë§ìŒ	.repartition(5~20)ë¡œ ì¡°ì ˆ, ë˜ëŠ” UPDATE ëŒ€ì‹  DELETE+INSERT ê³ ë ¤
ë³‘ë ¬ì€ ëŠ˜ì—ˆëŠ”ë° ëŠë¦¼	ClickHouse ìì²´ì˜ QPS í•œê³„ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë³‘ëª© í™•ì¸
partition ìˆ˜ ë„ˆë¬´ ì ìŒ	.coalesce(n)ì€ ë³‘ë ¬ ì¤„ì´ê¸°ìš© (ì£¼ì˜í•´ì„œ ì‚¬ìš©)



---

í•„ìš”í•˜ì‹œë©´:

ë©”ì‹œì§€ëŸ‰ì— ë”°ë¼ ë™ì  .repartition() ì ìš©

íŒŒí‹°ì…˜ ìˆ˜ ìë™ íŠœë‹ ë°©ë²•

Spark ë³‘ë ¬ ì²˜ë¦¬ ë¡œê¹… ë°©ë²•


ë„ ë„ì™€ë“œë¦´ê²Œìš”!
ì§€ê¸ˆ êµ¬ì¡°ì— .repartition(n)ì„ DataFrame ë‹¨ê³„ì—ë§Œ ì¶”ê°€í•˜ë©´ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë²½íˆ ì ìš©ë©ë‹ˆë‹¤ âœ…

