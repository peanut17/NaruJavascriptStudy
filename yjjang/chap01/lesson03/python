
인터넷이 안 되는 폐쇄망 Windows 11 PC에 PySpark를 설치하고 실행하려면, 필요한 패키지를 외부에서 다운로드한 후 USB 또는 네트워크 공유를 통해 옮겨서 설치해야 합니다. 아래 단계대로 진행하세요.


---

1. 필요 패키지 및 파일 다운로드 (인터넷 연결된 PC에서)

(1) Python 설치 파일 다운로드

1. Python 공식 웹사이트에서 Python 3.8~3.10 버전을 다운로드합니다.

PySpark는 최신 Python 버전과의 호환성이 떨어질 수 있으므로 3.10 이하 버전을 추천합니다.




(2) Java JDK 다운로드

1. Adoptium 또는 Oracle JDK에서 Java 8, 11, 17 중 하나를 다운로드합니다.

권장: JDK 11 (LTS 버전)




(3) Hadoop Winutils 다운로드

1. GitHub - winutils에서 Hadoop 3.2.2 버전의 winutils.exe를 다운로드합니다.

hadoop-3.2.2/bin/winutils.exe 파일을 복사하여 폐쇄망 PC로 옮깁니다.




(4) Spark 다운로드

1. Apache Spark 공식 웹사이트에서 Spark 3.x (Pre-built for Hadoop 3.2 or later) 버전을 다운로드합니다.


2. 압축을 풀어서 폐쇄망 PC에 복사합니다.



(5) PySpark 관련 Python 패키지 다운로드

폐쇄망에서는 pip install을 사용할 수 없으므로, 인터넷이 되는 PC에서 필요한 패키지를 미리 다운로드해야 합니다.

1. pip download 명령어를 사용하여 필요한 패키지 다운로드:

mkdir pyspark_offline
cd pyspark_offline
pip download pyspark pandas numpy py4j findspark


2. pyspark_offline 폴더를 폐쇄망 PC로 복사합니다.




---

2. 폐쇄망 PC에서 PySpark 환경 설정

(1) Python 설치

1. 복사한 python-3.x.x.exe를 실행하여 Python을 설치합니다.


2. 설치 시 "Add Python to PATH" 옵션을 체크하세요.



(2) Java JDK 설치

1. 복사한 JDK 설치 파일을 실행하여 Java를 설치합니다.


2. 환경 변수 설정 (JAVA_HOME 등록)

"환경 변수 편집"에서 JAVA_HOME을 추가하고 JDK 경로를 입력합니다.

JAVA_HOME = C:\Program Files\Eclipse Adoptium\jdk-11.0.x

Path에 다음 경로를 추가합니다.

C:\Program Files\Eclipse Adoptium\jdk-11.0.x\bin




(3) Spark 압축 해제 및 환경 변수 설정

1. 다운로드한 Spark를 C:\spark 경로에 압축 해제합니다.


2. 환경 변수 설정:

SPARK_HOME 추가

SPARK_HOME = C:\spark

HADOOP_HOME 추가

HADOOP_HOME = C:\hadoop

Path에 다음 추가:

C:\spark\bin
C:\hadoop\bin




(4) Hadoop Winutils 설정

1. C:\hadoop\bin\winutils.exe 파일이 있는지 확인합니다.


2. 권한 설정 (관리자 권한으로 실행 필요)

C:\hadoop\bin\winutils.exe chmod -R 777 C:\tmp



(5) PySpark 패키지 설치

1. 복사한 pyspark_offline 폴더로 이동

cd C:\pyspark_offline


2. 오프라인으로 패키지 설치

pip install --no-index --find-links=. pyspark pandas numpy py4j findspark




---

3. PySpark 실행 및 테스트

(1) PySpark 실행 확인

1. 명령 프롬프트 (CMD)에서 다음 명령어 실행

pyspark


2. Spark 세션이 정상적으로 실행되면 설치 성공



(2) Python 코드로 실행 테스트

Python에서 PySpark를 사용할 수 있는지 확인하려면 아래 코드를 실행해보세요.

import findspark
findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Test").getOrCreate()

df = spark.createDataFrame([(1, "Alice"), (2, "Bob")], ["id", "name"])
df.show()

이제 인터넷이 없는 폐쇄망 환경에서도 PySpark를 실행할 수 있습니다!



인터넷이 없는 폐쇄망 환경에서 Python과 데이터 분석에 필요한 라이브러리를 설치하려면, 인터넷이 연결된 PC에서 미리 패키지를 다운로드한 후 USB나 네트워크를 통해 옮겨 설치해야 합니다.


---

1. 필요한 도구 준비

(1) Python 다운로드 및 설치 (필요한 경우)

1. 인터넷이 되는 PC에서 Python 공식 웹사이트에서 Python 3.8~3.10 버전을 다운로드합니다.


2. 설치 후, 설치 경로를 환경 변수에 추가합니다.

설치 시 "Add Python to PATH" 옵션을 체크하세요.




(2) pip 최신 버전으로 업데이트 (인터넷 환경에서)

python -m pip install --upgrade pip

이후, 폐쇄망 PC로 pip을 옮길 필요는 없습니다.


---

2. 필요한 라이브러리 다운로드 (인터넷 PC에서)

인터넷이 되는 PC에서 pip download를 이용해 필요한 라이브러리를 미리 다운로드합니다.

(1) 패키지 다운로드 폴더 생성

mkdir offline_packages
cd offline_packages

(2) 데이터 분석 기본 라이브러리 다운로드

아래 명령어를 실행하여 필요한 라이브러리를 다운로드합니다.

pip download numpy pandas matplotlib seaborn scikit-learn scipy statsmodels openpyxl xlrd jupyter

추가 패키지가 필요하면 여기에 포함하세요

예) pip download tensorflow torch opencv-python

(3) 패키지를 폐쇄망 PC로 복사

다운로드된 .whl 파일과 .tar.gz 파일을 USB 또는 네트워크 공유 폴더를 이용하여 폐쇄망 PC로 복사합니다.


---

3. 폐쇄망 PC에서 패키지 설치

폐쇄망 PC에서 터미널 또는 명령 프롬프트를 열고, 복사한 폴더로 이동합니다.

cd C:\offline_packages

그런 다음, 아래 명령어를 실행하여 오프라인으로 패키지를 설치합니다.

pip install --no-index --find-links=. numpy pandas matplotlib seaborn scikit-learn scipy statsmodels openpyxl xlrd jupyter

--no-index 옵션을 사용하면 인터넷 연결 없이 로컬 폴더에서만 설치합니다.


---

4. 설치 확인 및 테스트

설치가 제대로 되었는지 확인하려면, Python을 실행한 후 아래 코드를 입력하세요.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import scipy
import statsmodels.api as sm

에러 없이 실행되면 정상적으로 설치된 것입니다.


---

5. (선택) Jupyter Notebook 실행 테스트

Jupyter Notebook을 설치한 경우, 아래 명령어를 실행하여 정상적으로 실행되는지 확인하세요.

jupyter notebook

웹 브라우저가 열리면 정상적으로 설치된 것입니다.


---

이제 폐쇄망에서도 Python과 데이터 분석 라이브러리를 사용할 수 있습니다!

